{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:crimson\">1. Naive data linkage without blocking</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv files\n",
    "google_products = pd.read_csv('google_small.csv')\n",
    "amazon_products = pd.read_csv('amazon_small.csv')\n",
    "true = pd.read_csv('amazon_google_truth_small.csv').sort_values(by='idAmazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jaccard similarity function\n",
    "def get_jaccard_sim(str1, str2):\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a common column for merge\n",
    "google_products['key'] = 1\n",
    "amazon_products['key'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the 2 dataframes\n",
    "joined = pd.merge(google_products, amazon_products, on='key').drop('key', axis=1)\n",
    "# calculate the scores for the names/titles using the jaccard index\n",
    "joined['name_score'] =joined.apply(lambda row: get_jaccard_sim(row['name'], row['title']), axis=1)\n",
    "# calculate the scores for the price similarities\n",
    "joined['price_score'] = joined.apply(lambda row: (min(row['price_x'], row['price_y'])/max(row['price_x'], row['price_y'])), axis=1)\n",
    "# calculate the final scores with the price weighted less as there is more chance of duplicates\n",
    "joined['final_score'] = joined['name_score'] + joined['price_score']/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold determined through trial and error, only concerned with values above this threshold\n",
    "THRESHOLD = 0.54\n",
    "joined = joined[joined['final_score'] > THRESHOLD]\n",
    "# take only the largest score for each amazonID for comparison\n",
    "joined = joined.sort_values(by='final_score', ascending=False).drop_duplicates(['idAmazon'])\n",
    "# create new dataframes for faster calculations\n",
    "predicted = joined.loc[:, ['idAmazon', 'idGoogleBase']].sort_values(by='idAmazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idAmazon</th>\n",
       "      <th>idGoogleBase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1931102953</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b00002s6sc</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1049...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b00004nhn7</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1843...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b000051sgq</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b000067fk7</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/3785...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idAmazon                                       idGoogleBase\n",
       "0  1931102953  http://www.google.com/base/feeds/snippets/1272...\n",
       "1  b00002s6sc  http://www.google.com/base/feeds/snippets/1049...\n",
       "2  b00004nhn7  http://www.google.com/base/feeds/snippets/1843...\n",
       "3  b000051sgq  http://www.google.com/base/feeds/snippets/1758...\n",
       "4  b000067fk7  http://www.google.com/base/feeds/snippets/3785..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe of tp values\n",
    "tp_values = []\n",
    "tp_df = predicted.merge(true, on=['idAmazon', 'idGoogleBase'])\n",
    "tp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.9147286821705426\n",
      "recall = 0.9076923076923077\n"
     ]
    }
   ],
   "source": [
    "# calculate precision and recall using tp, fp, fn\n",
    "tp = len(tp_df)\n",
    "fp = len(predicted) - tp\n",
    "fn = len(true) - tp\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "print(f'precision = {precision}')\n",
    "print(f'recall = {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**DISCUSSION**</span><br>\n",
    "After testing, it was decided that manufacturer and description should not be used in linkage as this lead to too much variance in results. \n",
    "\n",
    "A jaccard index, which is used to measure the overlap of two strings, was used to compare the strings in order to get a score based on the similarity of the titles, this was used as titles/names will usually be quite similar across platforms and the jaccard index will usually lead to accurate results while measuring short titles such as the ones in these datasets.\n",
    "\n",
    "to calculate the similarity of the prices, i took the minimum value of the two and divided that by the maximum value of the two, this leads to creating a (smaller than one) score based on how much smaller the first number is from the second.\n",
    "\n",
    "The final score was decided by summing the name score and half the price score. The reason half the price score was used was due to it being a less accurate representation of similarity (multiple items can have the same/similar price) in comparison to name which will very rarely have the same/similar values.\n",
    "\n",
    "The threshold for determining the scores was done through trial and error in order to get the best balance between precision and recall. We also only accounted for the idAmazon's with the highest final scores when comparing with our truth dataset as there can only be one true match for each id, this sufficienty improved the performance of our linkage.\n",
    "\n",
    "The performance shows us precision ~ 0.915 and recall ~ 0.908. These values both appear to be very good as we have a very high rate of correct linkage between our datasets while still covering a large amount of the true values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:crimson\">1. Blocking for efficient data linkage</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes from datasets\n",
    "google_products = pd.read_csv('google.csv')\n",
    "amazon_products = pd.read_csv('amazon.csv')\n",
    "true = pd.read_csv('amazon_google_truth.csv')\n",
    "true = true.rename(columns={'idGoogleBase':'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101515.55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess price data\n",
    "# assume prices are in gbp already so no need to apply exchange rate\n",
    "google_products['price'] = google_products['price'].str.replace('gbp', '')\n",
    "amazon_products['price'] = amazon_products['price'].replace('gbp', '')\n",
    "google_products['price'] = pd.to_numeric(google_products['price'])\n",
    "amazon_products['price'] = pd.to_numeric(amazon_products['price'])\n",
    "amazon_products['price'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blocks and labels for price and allocate each id to a block\n",
    "blocks = [0, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 100000, 1000000]\n",
    "labels = list(range(1,12))\n",
    "amazon_products['bin'] = pd.cut(amazon_products.price, bins=blocks, include_lowest=True, right=True, labels=labels)\n",
    "google_products['bin'] = pd.cut(google_products.price, bins=blocks, include_lowest=True, right=True, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idAmazon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1056...</td>\n",
       "      <td>b000aoz7hw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1290...</td>\n",
       "      <td>b000hcl5sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1823...</td>\n",
       "      <td>b000jx1kgq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1826...</td>\n",
       "      <td>b000h25yr0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1825...</td>\n",
       "      <td>b0009yegpc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id    idAmazon\n",
       "0  http://www.google.com/base/feeds/snippets/1056...  b000aoz7hw\n",
       "1  http://www.google.com/base/feeds/snippets/1290...  b000hcl5sm\n",
       "2  http://www.google.com/base/feeds/snippets/1823...  b000jx1kgq\n",
       "3  http://www.google.com/base/feeds/snippets/1826...  b000h25yr0\n",
       "4  http://www.google.com/base/feeds/snippets/1825...  b0009yegpc"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a common column to merge on\n",
    "google_products['key'] = 1\n",
    "amazon_products['key'] = 1\n",
    "\n",
    "matches = []\n",
    "fp = 0\n",
    "\n",
    "# find matches within the ground truth and each block\n",
    "for i in labels:\n",
    "    amazon = amazon_products.loc[amazon_products['bin'] == i]\n",
    "    google = google_products.loc[google_products['bin'] == i]\n",
    "    joined = pd.merge(google, amazon, on='key').drop('key', axis=1).loc[:, ['id', 'idAmazon']]\n",
    "    df = joined.merge(true, on=['idAmazon', 'id'])\n",
    "    fp += (len(joined) - len(df))\n",
    "    matches.append(df)\n",
    "                \n",
    "matches = pd.concat(matches)\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair completeness = 0.7876923076923077\n",
      "Reduction ratio = 0.8019180184478734\n"
     ]
    }
   ],
   "source": [
    "# calculate PC and RR\n",
    "tp = len(matches)\n",
    "fn = len(true) - tp\n",
    "PC = tp/(tp+fn)\n",
    "n = len(amazon_products)*len(google_products)\n",
    "RR = 1-(tp+fp)/n\n",
    "print(f'Pair completeness = {PC}')\n",
    "print(f'Reduction ratio = {RR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**DISCUSSION**</span><br>\n",
    "\n",
    "It was decided to use a blocking method based on price ranges. This was decided by somewhat of a trial and error process, as well as deduction, I originally attempted to block with n-grams on title but this lead to having far too many blocks and turned out to be a poor way to block these datasets. The sizes of the price blocks were determined by prior knowledge of how items are generally priced and the largest block size was determined by calling .max() on the prices of both dataframes. I also decided to not impute the 0 values of price in the amazon.csv file to anything, this is because imputing it to the mean/median would be improper as we are solely blocking on price and there is large variance in prices.\n",
    "\n",
    "This blocking method seemed to work quite well, both the PC and RR are around 80% which is quite high. These results are not as good as the results produced from the measures in the naive-linkage implemented earlier but this is to be expected as blocking is generally a less precise method as it prioritises speed and security over results. If a naive linkage method was used here, it would be very slow as there are millions of possible combinations between the datasets, using blocking significantly sped up the process.\n",
    "\n",
    "pair completeness measures the coverage of our values from within the truth dataset, with our value being ~80%, we have quite a good recall considering that blocking was used this could potentially be increased further by adding a second conjunction to the blocking scheme. Reduction ratio is a measure of the effectiveness of data reduction, here we also have a value of ~80% which means that we have a high ratio of data ingested to data matched.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
